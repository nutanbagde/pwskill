{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "036c6da5-a9b5-4535-96ab-722f1aa432b9",
   "metadata": {},
   "source": [
    "## Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f4b949-5f80-4782-97c3-329c3c78a67e",
   "metadata": {},
   "source": [
    "### Ans:- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4f5863-90ba-40a4-987e-ad90bab18645",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to transform numerical features to a common scale. It scales the values of features to a specified range, typically between 0 and 1, while preserving the relative relationships between the values. Min-Max scaling is especially useful when the features have different scales and ranges, as it ensures that each feature contributes equally to the analysis or model training.\n",
    "\n",
    "The formula for Min-Max scaling is as follows:\n",
    "\n",
    "__Xscaled__ =( __X__ - Xmin /( Xmax - Xmin)\n",
    "\n",
    "where:\n",
    "* __X__ is the scaled feature value.\n",
    "* Xmin is the minimum value of the feature.\n",
    "* Xmax is the maximum value of the feature.\n",
    "* Xscaled is the scaled feature value.\n",
    "Here's an example to illustrate the application of Min-Max scaling:\n",
    "\n",
    "Suppose you have a dataset of house prices with two features: \"square footage\" and \"number of bedrooms.\" The square footage values range from 800 to 2500, and the number of bedrooms varies from 1 to 4.\n",
    "\n",
    "| Square Footage | Bedrooms | Price ($)\n",
    "|--------------- |----------|----------\n",
    "| 1200           | 2        | 150000\n",
    "| 1800           | 3        | 250000\n",
    "| 2200           | 4        | 300000\n",
    "\n",
    "To apply Min-Max scaling to the \"Square Footage\" feature, you would calculate the scaled values using the formula:\n",
    "\n",
    "__Xscaled__ =( __X__ - Xmin /( Xmax - Xmin)\n",
    "\n",
    "For \"Square Footage\":\n",
    "\n",
    "Xmin =800\n",
    "Xmax =2500\n",
    "\n",
    "Applying the formula for each square footage value:\n",
    "\n",
    "* For 1200 square feet: Xscaled = (1200-800)/(2500-800) = 0.25 (Approximatelly)\n",
    "* For 1800 square feet: Xscaled = (1800-800)/(2500-800) = 0.5 (Approximatelly)\n",
    "* For 2200 square feet: Xscaled = (2200-800)/(2500-800) = 0.75 (Approximatelly)\n",
    "\n",
    "After scaling the \"Square Footage\" feature, the dataset might look like this:\n",
    "\n",
    "\n",
    "| Scaled Square Footage | Bedrooms | Price ($)\n",
    "|---------------------- |----------|----------\n",
    "| 0.25                   | 2        | 150000\n",
    "| 0.5                    | 3        | 250000\n",
    "| 0.75                   | 4        | 300000\n",
    "\n",
    "\n",
    "By applying Min-Max scaling, both \"Square Footage\" and \"Bedrooms\" features are now within the range of 0 to 1, making them comparable and suitable for various analyses and machine learning algorithms that require normalized input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890291af-7695-4a11-af74-61bbc896b495",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff4463c-f3ae-4327-bac4-b843e4d3ffc6",
   "metadata": {},
   "source": [
    "## Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd306b7-342a-4667-b099-debd2e70d5a5",
   "metadata": {},
   "source": [
    "### Ans:- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20c5cb9-b435-4e3c-82a0-a052431deb73",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as \"Normalization,\" is a feature scaling method that scales the values of a feature to have a magnitude of 1 while preserving the direction of the data points in the feature space. It's particularly useful when the scale of features varies significantly, and you want to ensure that all features contribute equally to the analysis or modeling process. Unlike Min-Max scaling, which brings the values within a specified range, the Unit Vector technique focuses on maintaining the relative direction of the data points.\n",
    "\n",
    "The formula for Unit Vector scaling is as follows:\n",
    "\n",
    "__Xnormalized = X/∥X∥__\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "* X is the original feature value.\n",
    "* ∥X∥ is the Euclidean norm (magnitude) of the feature vector.\n",
    "\n",
    "Here's an example to illustrate the application of the Unit Vector technique:\n",
    "\n",
    "Suppose you have a dataset of houses with two features: \"square footage\" and \"number of bedrooms.\" You want to normalize these features using the Unit Vector technique.\n",
    "\n",
    "\n",
    "Original dataset:\n",
    "\n",
    "yaml\n",
    "Copy code\n",
    "| Square Footage | Bedrooms\n",
    "|--------------- |----------\n",
    "| 1200           | 2\n",
    "| 1800           | 3\n",
    "| 2200           | 4\n",
    "\n",
    "To apply the Unit Vector technique to the \"Square Footage\" feature, you would calculate the normalized values using the formula:\n",
    "\n",
    "__Xnormalized = X/∥X∥__\n",
    "\n",
    " \n",
    "\n",
    "For \"Square Footage\":\n",
    "\n",
    "* For 1200 square feet: Xnormalized = (1200/sqrt(sqr(1200))) = 0.577\n",
    "* For 1800 square feet: Xnormalized = (1800/sqrt(sqr(1800))) = 0.577\n",
    "* For 2200 square feet: Xnormalized = (2200/sqrt(sqr(2200))) = 0.577\n",
    "\n",
    "After normalizing the \"Square Footage\" feature, the dataset might look like this:\n",
    "\n",
    "| Normalized Square Footage | Bedrooms\n",
    "|-------------------------- |----------\n",
    "| 0.577                     | 2\n",
    "| 0.577                     | 3\n",
    "| 0.577                     | 4\n",
    "\n",
    "\n",
    "In this example, the magnitude of the \"Square Footage\" feature vector is normalized to 1, while the relative relationships between the data points remain the same. The Unit Vector technique ensures that each data point in the feature space has the same magnitude, making the features comparable and suitable for analyses or modeling algorithms that require normalized input data.\n",
    "\n",
    "Differing from Min-Max scaling, the Unit Vector technique doesn't change the range of feature values but rather focuses on their direction, ensuring that the data points maintain their relationships without bias towards larger or smaller values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1bb541-eda1-4f05-a57e-29136c8af99e",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0282c570-6d6a-4bb7-9f99-c2b5fb3c580b",
   "metadata": {},
   "source": [
    "## Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6cbc78-5793-4317-9ea1-a2673277e890",
   "metadata": {},
   "source": [
    "### Ans:- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92183f33-f965-4f2a-a2df-1c5aeb89e827",
   "metadata": {},
   "source": [
    "\n",
    "PCA, which stands for Principal Component Analysis, is a dimensionality reduction technique used to transform a high-dimensional dataset into a lower-dimensional space while preserving as much of the original variability as possible. It achieves this by identifying the principal components, which are orthogonal vectors that capture the most significant patterns or directions of variation in the data.\n",
    "\n",
    "PCA is commonly used for tasks like data visualization, noise reduction, and feature extraction. By projecting the data onto a smaller set of principal components, PCA can reduce the computational complexity of subsequent analyses and potentially improve model performance.\n",
    "\n",
    "Here's how PCA works:\n",
    "\n",
    "Standardize Data: If necessary, standardize the data by subtracting the mean from each feature and dividing by the standard deviation. This ensures that all features are on the same scale.\n",
    "\n",
    "Compute Covariance Matrix: Calculate the covariance matrix of the standardized data. The covariance matrix represents the relationships between different features.\n",
    "\n",
    "Calculate Eigenvalues and Eigenvectors: Compute the eigenvalues and corresponding eigenvectors of the covariance matrix. Eigenvectors represent the directions of maximum variance in the data, and eigenvalues indicate the amount of variance captured in each eigenvector.\n",
    "\n",
    "Sort Eigenvalues: Sort the eigenvalues in descending order. The eigenvectors corresponding to the largest eigenvalues are the principal components that capture the most variance.\n",
    "\n",
    "Choose Principal Components: Select the top \n",
    "�\n",
    "k principal components that collectively capture a significant portion (e.g., 95%) of the total variance.\n",
    "\n",
    "Transform Data: Project the original data onto the selected principal components to create a lower-dimensional representation of the data.\n",
    "\n",
    "Here's an example illustrating the application of PCA for dimensionality reduction:\n",
    "\n",
    "Suppose you have a dataset of house attributes: \"size,\" \"number of bedrooms,\" \"number of bathrooms,\" \"garage size,\" and \"price.\"\n",
    "Original dataset:\n",
    "\n",
    "| Size | Bedrooms | Bathrooms | Garage Size | Price\n",
    "|----- |--------- |---------- |------------|------\n",
    "| 1500 | 3        | 2         | 2          | 250000\n",
    "| 2000 | 4        | 3         | 2          | 320000\n",
    "| 1800 | 3        | 2         | 1          | 280000\n",
    "| ...  | ...      | ...       | ...        | ...\n",
    "\n",
    "To apply PCA for dimensionality reduction, you would follow these steps:\n",
    "\n",
    "1. Standardize the data (if necessary).\n",
    "2. Calculate the covariance matrix.\n",
    "3. Compute eigenvalues and eigenvectors.\n",
    "4. Sort eigenvalues in descending order.\n",
    "5. Choose the top k principal components (e.g., k=2).\n",
    "6. Transform the data using the selected principal components.\n",
    "In this example, let's assume that the first two principal components capture around 95% of the total variance. After applying PCA, your reduced-dimensional dataset might look like this:\n",
    "\n",
    "Reduced dataset:\n",
    "\n",
    "| Principal Component 1 | Principal Component 2\n",
    "|---------------------- |----------------------\n",
    "| -0.5                   | 0.3\n",
    "| 1.2                    | -0.7\n",
    "| 0.0                    | 0.1\n",
    "| ...                    | ...\n",
    "\n",
    "You've successfully transformed the original data into a lower-dimensional representation while retaining most of the variance. This reduced dataset can now be used for visualization, analysis, or feeding into machine learning models with reduced computational complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a434fc2-3b9c-4162-9af3-467c1cb6b387",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebc15a9-0336-491d-9162-cdf1a59c7a36",
   "metadata": {},
   "source": [
    "## Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1f92d3-d5c6-44ad-8ce9-63d785c8bf16",
   "metadata": {},
   "source": [
    "### Ans:- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0f48c8-c904-4201-8518-69a6e5450e81",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) and feature extraction are closely related concepts. PCA can be used as a feature extraction technique to create new features (principal components) that capture the most important information from the original features. This can help reduce the dimensionality of the dataset while retaining the most relevant information for subsequent analysis or modeling.\n",
    "\n",
    "Here's how PCA can be used for feature extraction:\n",
    "\n",
    "__Step 1: Data Preprocessing:__\n",
    "Prepare your dataset by handling missing values, encoding categorical variables, and standardizing the features to have zero mean and unit variance.\n",
    "\n",
    "__Step 2: Apply PCA:__\n",
    "Apply PCA to the standardized dataset to create new features (principal components) that capture the variance in the original data. These principal components are linear combinations of the original features and are orthogonal to each other.\n",
    "\n",
    "__Step 3: Select Principal Components:__\n",
    "Choose the top k principal components that capture a significant portion of the total variance (e.g., 95%). These k principal components will serve as the new extracted features.\n",
    "\n",
    "__Step 4: Use Extracted Features:__\n",
    "The k selected principal components are used as the new features for subsequent analysis, such as visualization, clustering, classification, or regression.\n",
    "\n",
    "Here's an example illustrating the use of PCA for feature extraction:\n",
    "\n",
    "Suppose you have a dataset of images, each represented as a vector of pixel values. Each image has a high-dimensional feature space, making analysis and modeling computationally expensive. You want to extract a lower-dimensional representation of the images while preserving their essential characteristics.\n",
    "\n",
    "Original dataset:\n",
    "\n",
    "| Image 1 | Image 2 | ... | Image N\n",
    "|-------- |-------- |-----|---------\n",
    "| [0, 1,   | [0, 1,   | ... | [0, 1,\n",
    "|  0, 0,   |  0, 1,   |     |  0, 0,\n",
    "|  1, 1,   |  1, 1,   |     |  1, 1,\n",
    "|  ...     |  ...     |     |  ...\n",
    "\n",
    "\n",
    "To apply PCA for feature extraction, you would follow these steps:\n",
    "\n",
    "1. Standardize the pixel values across all images.\n",
    "2. Apply PCA to the standardized image data.\n",
    "3. Choose the top k principal components (e.g.,k=50) that capture most of the image variance.\n",
    "4. Use the k selected principal components as the new features.\n",
    "In this example, you've reduced the dimensionality of the image data while preserving the most important information. The new features (principal components) can be used for various tasks, such as image classification, clustering, or visualization.\n",
    "\n",
    "Keep in mind that the choice of the number of principal components (k) depends on the trade-off between dimensionality reduction and information preservation. Using fewer principal components reduces dimensionality but may result in some loss of information, while using more principal components retains more information but may not provide as much dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d42338f-a71a-49ff-a074-f6db6477bab5",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c901fc4-cb62-4790-b9fe-0e42f109509e",
   "metadata": {},
   "source": [
    "## Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7862bef3-2fe1-48eb-b799-a9ffbe2c2c19",
   "metadata": {},
   "source": [
    "### Ans:- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e130f993-2179-44ba-b8b0-22743ef33353",
   "metadata": {},
   "source": [
    "To preprocess the data for building a recommendation system for a food delivery service using Min-Max scaling, follow these steps:\n",
    "\n",
    "Step 1: Load and Understand the Data:\n",
    "Load the dataset containing features like price, rating, and delivery time. Understand the data's distribution and range for each feature.\n",
    "\n",
    "Step 2: Separate Features:\n",
    "Separate the features you want to scale from other relevant columns in the dataset.\n",
    "\n",
    "Step 3: Calculate Min and Max:\n",
    "Calculate the minimum (Xmin) and maximum (Xmax) values for each feature you want to scale. This can be done using numpy or pandas functions.\n",
    "\n",
    "Step 4: Apply Min-Max Scaling:\n",
    "Apply the Min-Max scaling formula to each value in the selected features to bring them within a common range (usually 0 to 1).\n",
    "\n",
    "Xscaled =( X - Xmin /( Xmax - Xmin)\n",
    "\n",
    "where:\n",
    "\n",
    "* X is the scaled feature value.\n",
    "* Xmin is the minimum value of the feature.\n",
    "* Xmax is the maximum value of the feature.\n",
    "* Xscaled is the scaled feature value. Here's an example to illustrate the application of Min-Max scaling:\n",
    "\n",
    "Step 5: Replace Scaled Values:\n",
    "Replace the original values in the dataset with the scaled values for the selected features.\n",
    "\n",
    "Step 6: Use the Preprocessed Data:\n",
    "The preprocessed data with scaled features can now be used for building the recommendation system. The scaled features ensure that the magnitude of each feature does not dominate the recommendation process, enabling a fair comparison between different features.\n",
    "\n",
    "After Min-Max scaling, the features like price, rating, and delivery time are scaled to a common range, allowing the recommendation system to consider them equally without one feature dominating the others due to its scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215aad9c-c413-4692-947e-2cf319ba5fe2",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5723d8-a5a1-4a3d-bcd3-7b4fe4ec77fe",
   "metadata": {},
   "source": [
    "## Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7f9bca-fa5a-4fca-8629-3cc4ec0df746",
   "metadata": {},
   "source": [
    "### Ans:- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9d9540-6599-4c9a-8d68-919a2b8165d9",
   "metadata": {},
   "source": [
    "Using PCA (Principal Component Analysis) to reduce the dimensionality of a dataset for predicting stock prices involves transforming the original features into a lower-dimensional representation while retaining the most significant patterns of variability. Here's how you would apply PCA to achieve dimensionality reduction for your stock price prediction project:\n",
    "\n",
    "Step 1: Data Preprocessing:\n",
    "Prepare your dataset by handling missing values, encoding categorical variables (if any), and standardizing the numerical features to have zero mean and unit variance. This step is important for ensuring that features are on the same scale and contribute equally during the PCA process.\n",
    "\n",
    "Step 2: Apply PCA:\n",
    "Apply PCA to the standardized dataset. This involves calculating the covariance matrix of the features and then computing the eigenvalues and eigenvectors of the covariance matrix.\n",
    "\n",
    "Step 3: Choose the Number of Principal Components:\n",
    "Determine how many principal components you want to retain. This decision depends on the trade-off between dimensionality reduction and the amount of variance you want to preserve. You can choose a fixed number of principal components or a percentage of total variance to retain (e.g., 95%).\n",
    "\n",
    "Step 4: Select Principal Components:\n",
    "Select the top \n",
    "�\n",
    "k principal components that correspond to the highest eigenvalues. These principal components capture the most significant variability in the original data.\n",
    "\n",
    "Step 5: Transform Data:\n",
    "Transform the original data using the selected principal components. This transformation results in a lower-dimensional representation of the data.\n",
    "\n",
    "Step 6: Use Transformed Data for Modeling:\n",
    "Use the transformed data with reduced dimensionality for training your stock price prediction model. You'll typically feed the transformed data into a machine learning algorithm such as regression, time series forecasting, or any other relevant technique.\n",
    "\n",
    "In this example, PCA is used to reduce the dimensionality of the dataset while retaining 95% of the variance. The transformed data (principal components) are then used for training a linear regression model to predict stock prices. The result is a model that considers the most important patterns of variability in the original features while reducing the computational complexity associated with high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e434dd-6227-48a4-9bd7-7dfee3163816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('stock_price_data.csv')\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.drop('price', axis=1)\n",
    "y = data['price']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.95)  # Retain 95% of the variance\n",
    "X_pca = pca.fit_transform(X_standardized)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16a9951-d7da-4d2f-aedb-26f6405b207e",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d19ef16-77a3-4e94-a255-e63ead00072f",
   "metadata": {},
   "source": [
    "## Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3100f42-3531-496d-950c-d527de3196dc",
   "metadata": {},
   "source": [
    "### Ans:- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dabaab1-a8e8-484e-8321-0bc4d41bbadb",
   "metadata": {},
   "source": [
    "To perform Min-Max scaling and transform the values in the dataset [1, 5, 10, 15, 20] to a range of -1 to 1, you can use the Min-Max scaling formula:\n",
    "\n",
    "Step 1: Calculate Min and Max:\n",
    "Calculate the minimum and maximum values of the dataset.\n",
    "Xmin=1\n",
    "Xmax=20\n",
    "\n",
    "Step 2: Apply Min-Max Scaling:\n",
    "Apply the Min-Max scaling formula to each value in the dataset:\n",
    "\n",
    "Xscaled=(X-Xmin)/(Xmax-Xmin)\n",
    "\n",
    "For each value in the original dataset:\n",
    "* For 1 : Xscaled = (1-1)/(20-1) = 0\n",
    "* For 5 : Xscaled = (5-1)/(20-1) = 0.25\n",
    "* For 10 : Xscaled = (10-1)/(20-1) = 0.45\n",
    "* For 15 : Xscaled = (15-1)/(20-1) = 0.7\n",
    "* For 20 : Xscaled = (20-1)/(20-1) = 1\n",
    "\n",
    "Step 3: Scale to -1 to 1:\n",
    "Now that the values are scaled between 0 and 1, you can scale them to the desired range of -1 to 1. You can achieve this by applying the following transformation:\n",
    "\n",
    "\n",
    "For each value in the original dataset:\n",
    "* For 1 : Xfinal = 2*0-1 =- 1\n",
    "* For 5 : Xfinal = 2*0.25-1 = -0.5\n",
    "* For 10 : Xfinal = 2*0.45-1 = -0.1\n",
    "* For 15 : Xfinal = 2*0.7-1 = 0.4\n",
    "* For 20 : Xfinal = 2*1-1 = 1\n",
    "\n",
    "So, the values in the dataset [1, 5, 10, 15, 20] after Min-Max scaling to a range of -1 to 1 would be approximately [-1, -0.5, -0.1, 0.4, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa777969-1f95-4b03-880c-b846f8a9b702",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.DataFrame([1, 5, 10, 15, 20])\n",
    "\n",
    "\n",
    "# Calculate min and max values for each feature\n",
    "feature_min = data.min()\n",
    "feature_max = data.max()\n",
    "\n",
    "# Apply Min-Max scaling using pandas\n",
    "scaled_data = (data - feature_min) / (feature_max - feature_min)\n",
    "\n",
    "# Alternatively, use MinMaxScaler from scikit-learn\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(subset_data)\n",
    "\n",
    "# Replace original values with scaled values in the dataset\n",
    "data = scaled_data\n",
    "\n",
    "# Now, the data is preprocessed with scaled features and can be used for the recommendation system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1133d05a-c900-44cf-82f1-51d0db3db415",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9614d594-5cf9-4aa1-8fb5-befa04171aa0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    20\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f35eab66-0322-425e-b338-e9a726ef4faa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n",
      "[0.21052632]\n",
      "[0.47368421]\n",
      "[0.73684211]\n",
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "for i in scaled_data:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9673ba6f-08ab-4b28-8d8b-af173d4bb89b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.21052632, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.47368421, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.73684211, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258bb9a1-099f-4cb0-a5a5-4f61564821dc",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8250b8e3-aeae-4280-858a-15300233b047",
   "metadata": {},
   "source": [
    "## Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cfe4e4-f6a4-4640-b834-af542ce01e7a",
   "metadata": {},
   "source": [
    "### Ans:- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c37a27-984e-41b3-bc48-883269181da8",
   "metadata": {},
   "source": [
    "Choosing the number of principal components to retain in PCA involves finding a balance between reducing dimensionality and retaining a sufficient amount of variance to accurately represent the original data. A common approach is to choose the number of principal components that collectively explain a significant portion of the total variance in the dataset. This can be achieved by looking at the explained variance ratio associated with each principal component.\n",
    "\n",
    "Here's a step-by-step process to determine the number of principal components to retain:\n",
    "\n",
    "Step 1: Standardize Data:\n",
    "Standardize the features (height, weight, age, gender, blood pressure) so that they have zero mean and unit variance. This step ensures that all features are on the same scale, which is important for PCA.\n",
    "\n",
    "Step 2: Apply PCA:\n",
    "Apply PCA to the standardized dataset and compute the eigenvalues and eigenvectors of the covariance matrix.\n",
    "\n",
    "Step 3: Calculate Explained Variance Ratio:\n",
    "Calculate the explained variance ratio for each principal component. The explained variance ratio of a principal component is the proportion of the total variance that it captures. This information is typically available in the explained_variance_ratio_ attribute of the PCA object.\n",
    "\n",
    "Step 4: Decide on Number of Principal Components:\n",
    "Decide how much variance you want to retain in your reduced dataset. A common threshold is to retain a certain percentage of the total variance, such as 95% or 99%. You can sum the explained variance ratios of the principal components until you reach your desired threshold.\n",
    "\n",
    "For example, if you find that the first two principal components explain 80% of the total variance, and you want to retain 95% of the variance, you might choose to retain the first two principal components.\n",
    "\n",
    "Keep in mind that the choice of the number of principal components can also depend on the specific goals of your analysis or modeling task. If dimensionality reduction is your main objective, you might be willing to retain fewer principal components. If retaining interpretability of features is important, you might choose to retain more principal components.\n",
    "\n",
    "In summary, the number of principal components to retain depends on the trade-off between dimensionality reduction and the amount of information you want to preserve. It's a good practice to visualize the explained variance ratio and determine the number of principal components that best suits your specific task and data characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387430c1-1748-479c-9a11-a7b1c2b634ab",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
