{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a262a79-ec85-4128-8d27-68ad297a3273",
   "metadata": {},
   "source": [
    "## Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce51d42-9d58-4c6e-84ac-ccb0e6e0c3a7",
   "metadata": {},
   "source": [
    "## Ans:- \n",
    "__Overfitting and underfitting__ are two common challenges in machine learning that relate to the model's ability to generalize from training data to new, unseen data.\n",
    "\n",
    "### 1. Overfitting:\n",
    "Overfitting occurs when a model learns the training data too well, capturing not only the underlying patterns but also the noise and random fluctuations present in the data. As a result, the model performs extremely well on the training data but poorly on new, unseen data.\n",
    "\n",
    "__Consequences of overfitting:__\n",
    "\n",
    "1. High training accuracy but poor testing/generalization accuracy.\n",
    "2. The model memorizes the training data instead of learning meaningful patterns, making it unable to generalize to new examples.\n",
    "3. Increased complexity of the model, leading to longer training times and potential resource constraints.\n",
    "__Mitigation strategies for overfitting:__\n",
    "\n",
    "1. Regularization: Introduce penalties on the complexity of the model during training to discourage it from fitting noise. Techniques like L1 regularization (Lasso) and L2 regularization (Ridge) are commonly used.\n",
    "2. Cross-Validation: Use techniques like k-fold cross-validation to evaluate the model's performance on multiple subsets of the data, ensuring that it doesn't overfit to a specific subset.\n",
    "3. Feature Selection: Choose relevant features and reduce dimensionality to prevent the model from fitting noise in irrelevant data.\n",
    "4. Early Stopping: Monitor the model's performance on a validation set and stop training when its performance starts deteriorating, indicating that it's starting to overfit.\n",
    "5. Ensemble Methods: Combine multiple models (e.g., Random Forests, Gradient Boosting) to reduce overfitting by leveraging the diversity of different models' predictions.\n",
    "6. Collect More Data: A larger dataset can help the model generalize better by exposing it to a wider variety of patterns and reducing the impact of noise.\n",
    "\n",
    "### 2.Underfitting:\n",
    "Underfitting occurs when a model is too simple to capture the underlying patterns in the training data. As a result, it performs poorly not only on the training data but also on new, unseen data.\n",
    "\n",
    "__Consequences of underfitting:__\n",
    "\n",
    "1. Low training accuracy and low testing/generalization accuracy.\n",
    "2. The model fails to learn the underlying relationships in the data, resulting in poor predictive performance.\n",
    "3. The model may lack complexity to represent the true patterns in the data.\n",
    "\n",
    "__Mitigation strategies for underfitting:__\n",
    "1. Increase Model Complexity: Use more complex models that have the capacity to capture intricate patterns in the data.\n",
    "2. Feature Engineering: Create more informative features or transform existing ones to expose hidden relationships.\n",
    "3. Add More Features: If possible, add more relevant features to give the model more information to learn from.\n",
    "4. Hyperparameter Tuning: Adjust hyperparameters like learning rate, regularization strength, or depth of trees to find the right balance between complexity and generalization.\n",
    "5. Ensemble Methods: Combine the predictions of multiple models to enhance their overall predictive power.\n",
    "6. Collect More Data: More data can help the model learn from a wider range of examples and reduce the chances of underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde4a1ed-498c-47bc-9481-f35982337e2e",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e4e2ef-35b3-4e16-9cf4-42c7bb962300",
   "metadata": {},
   "source": [
    "## Q2: How can we reduce overfitting? Explain in brief."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f9f6c2-7e79-4f3b-8737-722f59373ce2",
   "metadata": {},
   "source": [
    "## Ans:- \n",
    "\n",
    "Reducing overfitting is crucial to ensure that a machine learning model can generalize well to new, unseen data. Here are some key strategies to mitigate overfitting:\n",
    "\n",
    "1. Regularization: Regularization techniques add penalties to the model's loss function based on the complexity of the model. This discourages the model from fitting noise in the data. Two common types of regularization are L1 (Lasso) and L2 (Ridge) regularization.\n",
    "2. Cross-Validation: Use techniques like k-fold cross-validation to evaluate the model's performance on multiple subsets of the data. This helps to assess how well the model generalizes across different data partitions and prevents overfitting to a specific subset.\n",
    "3. Early Stopping: Monitor the model's performance on a validation set during training. If the validation performance starts to degrade, stop training to prevent the model from fitting noise and overfitting.\n",
    "4. Feature Selection: Choose relevant features and eliminate irrelevant ones. Reducing the dimensionality of the data can prevent the model from fitting noise in unnecessary features.\n",
    "5. Ensemble Methods: Combine predictions from multiple models to improve generalization. Techniques like Random Forests and Gradient Boosting create diverse models that collectively perform better and are less prone to overfitting.\n",
    "6. Reducing Model Complexity: Use simpler models with fewer parameters if they are sufficient for capturing the underlying patterns. Complex models are more prone to overfitting.\n",
    "7. Data Augmentation: Create synthetic data by applying transformations like rotation, translation, or noise addition to the original data. This provides the model with a more diverse training set and helps it learn better generalizable features.\n",
    "8. Dropout: In neural networks, dropout involves randomly deactivating a fraction of neurons during training. This prevents the network from relying too heavily on specific neurons and encourages more robust learning.\n",
    "9. Regularization Terms: Introduce additional terms in the loss function that penalize specific patterns, behaviors, or interactions in the model. This can help shape the model's learning process and prevent it from overfitting.\n",
    "10. Collect More Data: Increasing the size of the dataset provides the model with a wider range of examples and reduces the impact of noise on the model's learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86a8b0a-f9c5-4661-8cec-241018779ad0",
   "metadata": {},
   "source": [
    "----\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998d32de-d23e-4786-a0a2-0f7376adbfd9",
   "metadata": {},
   "source": [
    "## Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea04adb-2345-423b-a292-605d4551501d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ans:- \n",
    "\n",
    "Underfitting occurs when a machine learning model is too simple to capture the underlying patterns present in the data. As a result, the model performs poorly not only on the training data but also on new, unseen data. Underfitting often stems from a lack of model complexity, inability to capture intricate relationships, or insufficient training.\n",
    "\n",
    "Scenarios where underfitting can occur in machine learning include:\n",
    "\n",
    "1. Insufficient Model Complexity:\n",
    "* Using a linear model to capture nonlinear relationships in the data.\n",
    "* Employing a model with too few layers or nodes in a neural network for tasks requiring complex feature representations.\n",
    "\n",
    "2. Limited Features:\n",
    "* Working with a small number of features that fail to represent the true complexity of the data.\n",
    "* Using a reduced set of features that discards valuable information.\n",
    "\n",
    "3. Insufficient Training:\n",
    "* Training a model for too few iterations or epochs, preventing it from learning the underlying patterns.\n",
    "* Using a small fraction of the available data for training, leading to inadequate exposure to the dataset's variations.\n",
    "\n",
    "4. Ignoring Data Quality:\n",
    "* Neglecting to preprocess or clean the data, resulting in noisy or inconsistent inputs that hinder model performance.\n",
    "* Using data with missing values without proper handling.\n",
    "\n",
    "5. Ignoring Interactions:\n",
    "* Failing to account for interactions or relationships between features, causing the model to miss important patterns.\n",
    "\n",
    "6. Underfitting in Classification:\n",
    "* Applying a simple binary classifier to a multi-class classification problem.\n",
    "* Using a classifier with limited decision boundaries for complex decision boundaries.\n",
    "\n",
    "7. Underfitting in Regression:\n",
    "* Employing a linear regression model for data with strong nonlinear relationships.\n",
    "* Using a polynomial regression model with a low polynomial degree for data that requires a higher degree to capture the trend.\n",
    "\n",
    "8. Ignoring Temporal Aspects:\n",
    "* Overlooking time-series trends and dynamics when modeling time-dependent data.\n",
    "\n",
    "9. Underfitting in Text Data:\n",
    "\n",
    "* Using a basic bag-of-words approach without considering the semantic relationships between words in text analysis.\n",
    "* Applying simple models to text data without leveraging techniques like word embeddings or deep learning.\n",
    "\n",
    "10. Ignoring Domain Knowledge:\n",
    "* Disregarding domain-specific knowledge or insights that could guide the selection of appropriate features or models.\n",
    "\n",
    "To mitigate underfitting, it's essential to consider the complexity of the problem, the nature of the data, and the goals of the analysis. This might involve using more sophisticated models, increasing model complexity, selecting relevant features, collecting more data, improving data quality, and incorporating domain knowledge where appropriate. Experimenting with different approaches and observing the model's performance can help in identifying and addressing underfitting issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e969754d-cb4e-4f45-b1ae-bdbc17d8d4a6",
   "metadata": {},
   "source": [
    "----\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf66394-3b1c-47fc-9595-2d724856bf6d",
   "metadata": {},
   "source": [
    "## Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7479d4bf-fa9d-4f41-bdde-30681f9c24a3",
   "metadata": {},
   "source": [
    "## Ans:- \n",
    "\n",
    "\n",
    "The bias-variance tradeoff is a fundamental concept in machine learning that helps us understand the relationship between two types of errors, bias and variance, which collectively affect a model's performance and its ability to generalize well to new, unseen data.\n",
    "\n",
    "1. Bias:\n",
    "\n",
    "Bias refers to the error introduced by approximating a real-world problem, which may be complex, by a simplified model. A model with high bias tends to make strong assumptions about the data, often resulting in oversimplification. High bias leads to systematic errors in predictions that consistently deviate from the true values. In other words, the model consistently misses important patterns in the data.\n",
    "\n",
    "2. Variance:\n",
    "\n",
    "Variance refers to the model's sensitivity to small fluctuations or noise in the training data. A model with high variance captures the noise and random variations in the training data, leading to a lack of generalization. High variance results in erratic, unstable predictions that can be significantly different for different training sets.\n",
    "\n",
    "2. Bias-Variance Tradeoff:\n",
    "\n",
    "The bias-variance tradeoff illustrates the tension between a model's ability to fit the training data well and its ability to generalize to new data. The goal is to find the right balance between bias and variance to achieve optimal model performance.\n",
    "\n",
    "* Relationship between Bias and Variance:\n",
    "\n",
    "High Bias, Low Variance: A model with high bias and low variance is overly simplistic and underfits the data. It fails to capture the complexities and patterns present in the data. However, the model's predictions are consistent across different training sets.\n",
    "\n",
    "* Low Bias, High Variance:\n",
    "\n",
    "A model with low bias and high variance fits the training data closely but captures noise and random fluctuations. It performs well on the training data but generalizes poorly to new data, as slight variations in the training set lead to drastically different predictions.\n",
    "\n",
    "__Effect on Model Performance :-__ \n",
    "\n",
    "1. High Bias:\n",
    "* Training error is relatively high.\n",
    "* Testing error (generalization error) is also high due to the model's inability to capture important patterns.\n",
    "Model is underfitting.\n",
    "2. High Variance:\n",
    "* Training error is low.\n",
    "* Testing error is high because of the model's sensitivity to data fluctuations.\n",
    "* Model is overfitting.\n",
    "3. Balancing Bias and Variance:\n",
    "* The goal is to find the \"sweet spot\" between bias and variance to achieve the best possible model performance on new data. This can involve:\n",
    "\n",
    "Selecting an appropriate model complexity that captures the true relationships in the data without fitting noise.\n",
    "Regularization techniques to manage model complexity and prevent overfitting.\n",
    "\n",
    "Using ensemble methods to combine predictions from multiple models, reducing variance while maintaining low bias.\n",
    "In summary, the bias-variance tradeoff highlights the need to manage the tradeoff between a model's ability to fit the training data and its ability to generalize to new data. Finding the right balance is essential for creating models that perform well on a wide range of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80df5ccd-e208-4758-888a-9adfc221a7d8",
   "metadata": {},
   "source": [
    "-------\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8d7656-dcb5-438c-be49-61fe3e3bfc8b",
   "metadata": {},
   "source": [
    "## Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2e3504-db85-4ce5-8ac9-2196566402cf",
   "metadata": {},
   "source": [
    "## Ans:- \n",
    "\n",
    "\n",
    "\n",
    "Detecting overfitting and underfitting in machine learning models is crucial for building models that generalize well to new, unseen data. Here are some common methods for detecting and addressing these issues:\n",
    "\n",
    "### 1. Visualizing Learning Curves:\n",
    "\n",
    "Learning curves plot the model's performance (e.g., accuracy or error) on both the training and validation datasets as a function of the number of training samples or epochs. Overfitting is indicated by a large gap between the training and validation curves, where the model performs well on the training data but poorly on the validation data.\n",
    "\n",
    "### 2. Cross-Validation:\n",
    "\n",
    "Cross-validation involves splitting the dataset into multiple subsets (folds) and training the model on different combinations of training and validation sets. If the model performs well on the training data but poorly on the validation data across multiple folds, it might be overfitting.\n",
    "\n",
    "### 3. Regularization:\n",
    "\n",
    "Regularization techniques like L1 and L2 regularization can help prevent overfitting by adding penalty terms to the model's loss function. These penalties discourage the model from fitting noise in the training data.\n",
    "\n",
    "### 4. Holdout Validation:\n",
    "\n",
    "Holdout validation involves splitting the dataset into a training set and a separate validation set. If the model's performance on the validation set is significantly worse than on the training set, it might be overfitting.\n",
    "\n",
    "### 5. Feature Selection:\n",
    "\n",
    "Overfitting can occur when the model tries to learn from noise or irrelevant features. Performing feature selection or dimensionality reduction can help reduce overfitting by focusing on the most relevant features.\n",
    "\n",
    "### 6. Early Stopping:\n",
    "\n",
    "Monitor the model's performance on a validation set during training and stop training when the validation performance starts to degrade. This prevents the model from memorizing the training data.\n",
    "\n",
    "### 7. Ensemble Methods:\n",
    "\n",
    "Ensemble methods like Random Forests and Gradient Boosting involve combining multiple models to make predictions. These methods can help mitigate overfitting by averaging out individual model biases.\n",
    "\n",
    "### 8. Hyperparameter Tuning:\n",
    "\n",
    "Hyperparameters control the behavior of the model. Tuning them properly can help strike a balance between overfitting and underfitting. Techniques like grid search or random search can help find optimal hyperparameters.\n",
    "\n",
    "### 9. Bias-Variance Tradeoff:\n",
    "\n",
    "Understanding the bias-variance tradeoff is crucial. Models with high bias (underfitting) have low complexity and might not capture the underlying patterns, while models with high variance (overfitting) capture noise in the data. Striking the right balance is essential.\n",
    "\n",
    "### 10. Outlier Detection and Handling:\n",
    "\n",
    "Outliers can heavily influence the training process, leading to overfitting. Identifying and handling outliers appropriately can help improve the model's generalization.\n",
    "\n",
    "### 11. Monitoring Test Set Performance:\n",
    "\n",
    "After model deployment, monitor its performance on a separate test set. If the performance drops significantly, it might indicate that the model is overfitting to the validation set.\n",
    "\n",
    "### 12. K-fold Cross-Validation:\n",
    "\n",
    "This method involves dividing the dataset into k subsets, training the model on k-1 subsets, and validating it on the remaining subset. This process is repeated k times, rotating the validation subset. The average performance across all folds can provide a better estimate of the model's generalization performance.\n",
    "\n",
    "Remember that underfitting occurs when the model is too simple to capture the underlying patterns, while overfitting occurs when the model is too complex and captures noise. It's important to strike a balance between these two extremes by using appropriate techniques and evaluating the model's performance on independent datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3aa4df-fd15-4c2a-b673-08085b8a9ac0",
   "metadata": {},
   "source": [
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a8a5c4-9e5d-410c-8490-54e9b1a77d32",
   "metadata": {},
   "source": [
    "## Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3247507f-4922-43b2-955c-582678070eae",
   "metadata": {},
   "source": [
    "## Ans:- \n",
    "\n",
    "Bias and variance are two critical concepts in machine learning that relate to a model's performance and generalization ability:\n",
    "\n",
    "### 1. Bias:\n",
    "Bias refers to the error due to overly simplistic assumptions in the learning algorithm. A high bias model tends to underfit the training data, meaning it doesn't capture the underlying patterns in the data, leading to poor training and validation performance.\n",
    "\n",
    "### 2. Variance:\n",
    "Variance refers to the model's sensitivity to small fluctuations in the training data. A high variance model is overly complex and captures noise in the training data, leading to good performance on the training data but poor generalization to new, unseen data.\n",
    "\n",
    "Comparison:\n",
    "\n",
    "|Sr.No.|Bias|Variance|\n",
    "|:-:|:-:|:-:|\n",
    "|1|High bias models are typically too simple and have low flexibility.|High variance models are overly complex and have high flexibility.|\n",
    "|2|They tend to miss important patterns and relationships in the data.|They tend to fit noise in the data, resulting in high sensitivity to training data fluctuations.|\n",
    "|3|High bias models have low training and validation performance.|High variance models can have excellent training performance but poor validation performance.|\n",
    "|4|They generalize poorly to both training and unseen data.|They generalize well to the training data but poorly to unseen data.|\n",
    "\n",
    "\n",
    "\n",
    "Examples:\n",
    "\n",
    "* High Bias (Underfitting) Model:\n",
    "\n",
    "Linear Regression with few features when the underlying relationship is highly nonlinear.\n",
    "A model that assumes all images contain the same object, regardless of actual content.\n",
    "This model would have low training accuracy and low validation accuracy, indicating poor fit to the data.\n",
    "* High Variance (Overfitting) Model:\n",
    "\n",
    "A decision tree with very deep levels, effectively memorizing the training data.\n",
    "A complex neural network with many layers and parameters for a small dataset.\n",
    "This model might have near-perfect training accuracy but significantly lower validation accuracy, indicating it's fitting noise.\n",
    "\n",
    "## Performance Comparison:\n",
    "\n",
    "* High Bias Model:\n",
    "\n",
    "Training Error: High\n",
    "Validation Error: High (similar to training error)\n",
    "Test Error: High (poor generalization)\n",
    "Overall Performance: Poor on all datasets\n",
    "\n",
    "* High Variance Model:\n",
    "\n",
    "Training Error: Low (captures training data noise)\n",
    "Validation Error: High (fails to generalize)\n",
    "Test Error: High (poor generalization)\n",
    "Overall Performance: Good on training, poor on unseen data\n",
    "To strike a balance between bias and variance, it's crucial to choose an appropriate model complexity, apply regularization techniques, collect more data if possible, and use techniques like cross-validation to select the best model. The goal is to find the optimal tradeoff between the two to achieve better generalization performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0304afab-51ba-419e-bbdb-89ace94d438f",
   "metadata": {},
   "source": [
    "------\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f85a5e-af6e-4c2f-9cee-ca57789be9d6",
   "metadata": {},
   "source": [
    "## Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81f9a8f-beef-43c6-b279-2b2f7be8b893",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ans:- \n",
    "\n",
    "\n",
    "Regularization is a set of techniques used in machine learning to prevent overfitting by adding a penalty term to the model's objective function. Overfitting occurs when a model learns noise in the training data and generalizes poorly to new data. Regularization methods aim to control the model's complexity and make it generalize better by discouraging overly complex parameter values.\n",
    "\n",
    "### Common Regularization Techniques:\n",
    "\n",
    "1. L1 Regularization (Lasso):\n",
    "\n",
    "* L1 regularization adds the absolute values of the model's coefficients as a penalty term to the loss function.\n",
    "* It encourages the model to have sparse coefficients, effectively performing feature selection by driving some coefficients to exactly zero.\n",
    "* L1 regularization is useful when there are many irrelevant features in the data.\n",
    "\n",
    "2. L2 Regularization (Ridge):\n",
    "\n",
    "* L2 regularization adds the squared magnitudes of the model's coefficients to the loss function.\n",
    "* It penalizes large coefficient values, effectively making them smaller.\n",
    "* L2 regularization is helpful in preventing extreme parameter values and can improve the model's generalization.\n",
    "\n",
    "3. Elastic Net Regularization:\n",
    "\n",
    "* Elastic Net combines L1 and L2 regularization by adding both absolute and squared coefficient terms to the loss function.\n",
    "* It provides a balance between feature selection (L1) and coefficient shrinkage (L2).\n",
    "* Elastic Net is suitable when there are many features, and some of them are correlated.\n",
    "\n",
    "4. Dropout (Neural Networks):\n",
    "\n",
    "* Dropout is a technique used specifically for neural networks.\n",
    "* During training, randomly selected neurons (along with their connections) are dropped out or \"switched off\" with a certain probability.\n",
    "* This prevents the network from relying too heavily on any individual neuron and encourages the network to learn more robust features.\n",
    "\n",
    "5. Early Stopping:\n",
    "\n",
    "* Early stopping is not a penalty-based regularization technique but involves monitoring the model's performance on a validation set during training.\n",
    "* Training is stopped when the validation performance starts to degrade, preventing the model from fitting noise.\n",
    "* This technique helps prevent overfitting by finding the optimal point where validation performance is best.\n",
    "\n",
    "6. Data Augmentation:\n",
    "\n",
    "* Data augmentation is a technique used to artificially increase the size of the training dataset by applying transformations to existing data.\n",
    "* It introduces variability into the training data, making the model more robust and reducing overfitting.\n",
    "\n",
    "7. Batch Normalization:\n",
    "\n",
    "* Batch normalization is a technique that helps stabilize the training of deep neural networks.\n",
    "* It normalizes the inputs to each layer in a mini-batch, reducing internal covariate shifts.\n",
    "* This can lead to improved generalization and faster convergence.\n",
    "\n",
    "Regularization techniques work by introducing constraints or penalties to the model's parameters, making them less prone to overfitting. The choice of regularization method depends on the problem, the model architecture, and the amount of available data. Applying the right type and amount of regularization can help improve a model's ability to generalize to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781fb440-ba5e-4b16-ac67-a27361d0ee5d",
   "metadata": {},
   "source": [
    "--------\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09de4c8f-c70f-46de-9e93-b475904ee327",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
