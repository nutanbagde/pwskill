{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06fda69b-4e76-428f-ae6e-cd3b399af465",
   "metadata": {},
   "source": [
    "## Q1. What is the Filter method in feature selection, and how does it work?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91c2a9e-7596-4b8c-93fa-ff0240970de8",
   "metadata": {},
   "source": [
    "## Ans: \n",
    "\n",
    "The filter method is a feature selection technique used in machine learning to select a subset of relevant features from the original set of features. It involves evaluating each feature independently, without considering the interaction between features or the specific machine learning algorithm being used. The goal of the filter method is to identify features that have the highest correlation or statistical significance with the target variable.\n",
    "\n",
    "Here's how the filter method works:\n",
    "\n",
    "1. __Calculate Feature Scores:__ For each feature in the dataset, a score is calculated based on a predefined criterion. This criterion could be a statistical test, correlation coefficient, or another measure of the relationship between the feature and the target variable.\n",
    "\n",
    "2. __Rank Features:__ The features are ranked based on their scores in descending order. Features with higher scores are considered more relevant or informative.\n",
    "\n",
    "3. __Select Top Features:__ A certain number of top-ranked features are selected for inclusion in the final feature subset. The number of selected features can be predetermined or chosen based on a threshold.\n",
    "\n",
    "4. __Remove Irrelevant Features:__ Features that are not selected are discarded from the dataset, resulting in a reduced feature space.\n",
    "\n",
    "Common criteria used in the filter method include:\n",
    "\n",
    "* __Correlation Coefficient:__ Measures the linear relationship between a feature and the target variable. Features with high absolute correlation coefficients are considered more relevant.\n",
    "\n",
    "* __ANOVA (Analysis of Variance):__ This statistical test assesses whether the means of a feature's values are significantly different across different categories of the target variable.\n",
    "\n",
    "* __Chi-Square Test:__ Applicable to categorical features and target variables, the chi-square test measures the independence between a feature and the target variable.\n",
    "\n",
    "* __Information Gain or Mutual Information:__ These metrics from information theory quantify the amount of information a feature provides about the target variable.\n",
    "\n",
    "* __Variance Threshold:__ Features with low variance might not contribute much information. This approach filters out features with variance below a certain threshold.\n",
    "\n",
    "Advantages of the filter method include its simplicity, speed, and independence from the choice of machine learning algorithm. However, it does not consider feature interactions and might miss features that are important in combination with other features. Therefore, it's often used as an initial step in feature selection to quickly reduce the dimensionality of the dataset before applying more sophisticated methods like wrapper methods or embedded methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268e7cfe-5ced-4cd5-9a15-6915169e5ead",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb7b682-7adc-4263-93f4-47cdfea409c6",
   "metadata": {},
   "source": [
    "## Q2. How does the Wrapper method differ from the Filter method in feature selection?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37560462-31e9-4d22-a776-ef84d63934bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ans: \n",
    "\n",
    "The Wrapper method and the Filter method are two different approaches for selecting a subset of relevant features in machine learning. They have distinct characteristics and workflows. Here's how they differ:\n",
    "\n",
    "## Filter Method:\n",
    "\n",
    "1. Independence: The Filter method evaluates each feature independently of others, considering only the relationship between each feature and the target variable.\n",
    "\n",
    "2. Speed: Filter methods are computationally efficient since they don't involve training a machine learning model. They are suitable for large datasets with many features.\n",
    "\n",
    "3. Scalability: They are generally more scalable as the number of features increases.\n",
    "\n",
    "4. Criteria: Filter methods use predefined statistical measures (e.g., correlation coefficient, ANOVA, chi-square) to rank and select features. These criteria might not capture complex feature interactions.\n",
    "\n",
    "5. Model Agnostic: The choice of machine learning algorithm is not considered during feature selection. Features are selected based on their individual relationships with the target variable.\n",
    "\n",
    "6. Limitations: Filter methods might not capture subtle feature interactions that contribute to predictive power. They can't address the \"curse of dimensionality\" where irrelevant features can still be selected.\n",
    "\n",
    "## Wrapper Method:\n",
    "\n",
    "1. Dependence: The Wrapper method evaluates feature subsets using a specific machine learning algorithm. It involves training and testing multiple models with different feature combinations to identify the best subset.\n",
    "\n",
    "2. Performance: Since Wrapper methods use the actual machine learning model for evaluation, they provide a more accurate assessment of feature subsets' performance.\n",
    "\n",
    "3. Computational Complexity: Wrapper methods involve training and evaluating multiple models, making them computationally more expensive, especially for datasets with many features.\n",
    "\n",
    "4. Feature Interactions: Wrapper methods consider feature interactions since they train models on subsets of features. They can capture nonlinear relationships that filter methods might miss.\n",
    "\n",
    "5. Model Selection: The choice of machine learning algorithm is crucial in Wrapper methods, as it can influence the selected feature subset. Different algorithms might result in different subsets.\n",
    "\n",
    "6. Optimization: The goal of the Wrapper method is to find the best subset of features that optimizes the model's performance based on a chosen evaluation metric.\n",
    "\n",
    "7. Overfitting Risk: There's a risk of overfitting, especially with small datasets, as the model might find the best subset for the training data but perform poorly on new data.\n",
    "\n",
    "In summary, the Filter method evaluates features independently using predefined criteria, whereas the Wrapper method involves training and testing machine learning models on different feature subsets. The choice between these methods depends on factors such as dataset size, computational resources, the complexity of feature interactions, and the ultimate goal of feature selection â€“ whether it's just dimensionality reduction (Filter) or optimizing model performance (Wrapper)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227b208f-31e1-4e53-8295-81434b91b224",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dfbb31-17b7-411d-9a39-cfe394c002ce",
   "metadata": {},
   "source": [
    "## Q3. What are some common techniques used in Embedded feature selection methods?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94704f94-1bd4-45fb-9fc0-ad8b36e903ab",
   "metadata": {},
   "source": [
    "## Ans:\n",
    "\n",
    "Embedded feature selection methods incorporate feature selection as a part of the model training process. These methods automatically select relevant features during the model training phase, aiming to find the best subset of features that optimize the model's performance. Here are some common techniques used in embedded feature selection methods:\n",
    "\n",
    "1. Lasso Regression (L1 Regularization):\n",
    "Lasso regression adds a penalty term to the linear regression loss function based on the absolute values of the feature coefficients. As a result, some feature coefficients are shrunk to zero, effectively performing feature selection. Features with non-zero coefficients are considered relevant by the model.\n",
    "\n",
    "2. Ridge Regression (L2 Regularization):\n",
    "Similar to Lasso, ridge regression adds a penalty term to the linear regression loss function. However, in this case, the penalty is based on the square of the feature coefficients. While ridge regression doesn't lead to explicit feature selection (coefficients are not exactly zero), it can help mitigate multicollinearity and reduce the impact of less relevant features.\n",
    "\n",
    "3. Elastic Net Regression:\n",
    "Elastic Net combines the L1 (Lasso) and L2 (ridge) penalties. This approach can provide a balance between feature selection and managing multicollinearity.\n",
    "\n",
    "4. Decision Trees and Random Forests:\n",
    "Decision trees and ensemble methods like random forests implicitly perform feature selection by making splits based on features that have the most predictive power. Features that are not relevant tend to have less impact on the tree's decision paths.\n",
    "\n",
    "5. Gradient Boosting Algorithms:\n",
    "Gradient boosting algorithms (e.g., XGBoost, LightGBM) can be used for feature selection by examining the importance scores assigned to features during the boosting process. Features with higher importance scores are considered more relevant.\n",
    "\n",
    "6. Recursive Feature Elimination (RFE) with Support Vector Machines (SVM):\n",
    "RFE is an iterative approach that starts with all features and removes the least important feature in each iteration. When combined with SVM, RFE selects features by considering their impact on the SVM's performance.\n",
    "\n",
    "7. Regularized Regression for Nonlinear Models:\n",
    "Methods like regularized logistic regression (e.g., Logistic Lasso) or regularized support vector machines extend the idea of regularization to nonlinear models, allowing for feature selection in more complex scenarios.\n",
    "\n",
    "8. Feature Importance in Tree-Based Models:\n",
    "Tree-based models like decision trees and random forests provide feature importance scores based on how often a feature is used for splits and how much those splits improve the model's performance.\n",
    "\n",
    "9. Neural Networks with L1 Regularization:\n",
    "In neural networks, L1 regularization can encourage certain weights (and hence corresponding features) to become exactly zero, effectively leading to feature selection.\n",
    "\n",
    "10. Recursive Feature Addition (RFA) and Other Sequential Methods:\n",
    "RFA starts with an empty set of features and incrementally adds the most useful feature at each step based on the model's performance. Other sequential methods consider adding or removing features based on their impact.\n",
    "\n",
    "These techniques leverage the inherent properties of the model and the regularization penalties to perform feature selection while simultaneously optimizing model performance. The choice of method depends on the nature of the data, the complexity of the problem, and the specific algorithm you're using.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1302a1-c5dc-41d9-8dd3-f1035961baf1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0d2b12-c329-44c8-b334-7b513afe22f3",
   "metadata": {},
   "source": [
    "## Q4. What are some drawbacks of using the Filter method for feature selection?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e7a49f-120d-4427-b634-0c257e5d1597",
   "metadata": {},
   "source": [
    "## Ans: \n",
    "\n",
    "While the Filter method for feature selection has its merits, it also comes with several drawbacks that need to be considered:\n",
    "\n",
    "1. Ignores Feature Interactions: The Filter method evaluates features independently without considering interactions between features. It might miss important features that contribute to the model's predictive power only when combined with other features.\n",
    "\n",
    "2. Lack of Model Context: The Filter method doesn't take into account the actual machine learning algorithm being used. Some features might be less relevant on their own but become crucial when used within the context of a specific algorithm.\n",
    "\n",
    "3. Insensitive to Target Variable: Some features might have high correlation with the target variable but are not genuinely informative. The Filter method might select features that show statistical significance but lack meaningful insights.\n",
    "\n",
    "4. Doesn't Address Overfitting: The Filter method focuses on the relationship between individual features and the target variable but doesn't explicitly address overfitting, especially when selecting too many features.\n",
    "\n",
    "5. No Guarantee of Optimality: The selected features might not lead to the optimal model performance. Other methods like wrapper methods or embedded methods might provide better results by considering feature interactions and model performance.\n",
    "\n",
    "6. Threshold Selection Challenge: Setting an appropriate threshold for feature selection can be challenging. Choosing a threshold that's too stringent might lead to important features being excluded, while a loose threshold might retain irrelevant features.\n",
    "\n",
    "7. Bias Towards Numerical Features: Many filter methods are designed for numerical features and might not work as effectively with categorical features or a mix of both.\n",
    "\n",
    "8. Sensitive to Scaling: Some filter methods rely on statistical measures that are sensitive to feature scaling. If features have different scales, it can affect the results of the filter method.\n",
    "\n",
    "9. Doesn't Account for Data Imbalance: The Filter method might not be well-suited for imbalanced datasets, where the majority class overwhelms the minority class. It can lead to selection bias towards the majority class.\n",
    "\n",
    "10. Limited Exploration of Feature Space: Filter methods explore the feature space based on predefined criteria, potentially missing out on combinations of features that could contribute to model performance.\n",
    "\n",
    "11. May Not Generalize Well: Since the Filter method doesn't consider the interaction between features and the model's characteristics, it might not generalize well to new and unseen data.\n",
    "\n",
    "In summary, while the Filter method offers a quick and computationally efficient way to reduce feature dimensionality, it has limitations in terms of capturing complex feature interactions, addressing overfitting, and ensuring optimal model performance. It's often used as a preliminary step in feature selection to quickly eliminate irrelevant features before applying more advanced techniques like wrapper methods or embedded methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62495bf7-f65f-4b37-afb5-230b35841c60",
   "metadata": {
    "tags": []
   },
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9f5194-29d5-4b7b-b45d-6f204e2deab5",
   "metadata": {},
   "source": [
    "## Q5. In which situations would you prefer using the Filter method over the Wrapper method for feature\n",
    "selection?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcba4f5d-6863-42f7-a736-0b53e7e51a8c",
   "metadata": {},
   "source": [
    "## Ans: \n",
    "\n",
    "The choice between using the Filter method and the Wrapper method for feature selection depends on various factors, including the nature of the problem, the dataset, computational resources, and the goals of the analysis. Here are some situations where you might prefer using the Filter method over the Wrapper method:\n",
    "\n",
    "1. Large Datasets: Filter methods are computationally efficient and scale well to large datasets. If you have a massive dataset with numerous features, the Filter method can be a faster way to perform an initial feature selection.\n",
    "\n",
    "2. Dimensionality Reduction: When your primary goal is to reduce the dimensionality of the dataset quickly, the Filter method can help eliminate irrelevant features without the computational cost of training and evaluating multiple models.\n",
    "\n",
    "3. Exploratory Analysis: In the early stages of data analysis, when you're exploring feature relationships and don't have a specific machine learning model in mind, the Filter method can provide a quick overview of feature relevance.\n",
    "\n",
    "4. Initial Feature Screening: If you have a large number of features and you want to narrow down the list before applying more resource-intensive methods like Wrapper methods or embedded methods, the Filter method can serve as a preliminary step.\n",
    "\n",
    "5. Non-Predictive Tasks: In situations where the goal is not prediction, but rather understanding relationships between individual features and the target variable, Filter methods can provide insights without the need for complex modeling.\n",
    "\n",
    "6. No Access to Algorithm-Specific Metrics: Some datasets might not have well-defined evaluation metrics that Wrapper methods require. In such cases, Filter methods provide a way to perform feature selection without needing to define a specific model evaluation criterion.\n",
    "\n",
    "7. Feature Ranking: If you're interested in ranking features based on their individual importance or relevance, Filter methods provide a straightforward way to do so without requiring model training.\n",
    "\n",
    "8. Resource Constraints: When computational resources are limited, and you can't afford the computational overhead of training multiple models for every possible feature subset, the Filter method can be a more feasible option.\n",
    "\n",
    "9. Quick Data Preprocessing: Filter methods can be applied as a preliminary step to quickly clean up the dataset before further preprocessing or model building.\n",
    "\n",
    "10. Baseline Model: As a starting point, you might use the Filter method to establish a baseline model with a reduced set of features. This can help you gauge the potential of your data before investing more effort in feature selection.\n",
    "\n",
    "In summary, the Filter method is particularly useful when you want a fast and efficient way to perform feature selection, especially in situations where computational resources are limited or when you're in the early stages of data exploration. However, for tasks that require optimizing model performance and capturing complex feature interactions, the Wrapper method or embedded methods might be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f544496d-1c4d-4deb-bee7-8c343df29498",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3411f8f6-aa9e-44a5-9b2d-7880565d0dff",
   "metadata": {},
   "source": [
    "## Q6. In a telecom company, you are working on a project to develop a predictive model for customer churn. You are unsure of which features to include in the model because the dataset contains several different ones. Describe how you would choose the most pertinent attributes for the model using the Filter Method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eee3c9-12c8-4bac-931c-a7a84dd5455a",
   "metadata": {},
   "source": [
    "## Ans: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c1e443-3bef-429f-8104-454de6b6bd61",
   "metadata": {},
   "source": [
    "To choose the most pertinent attributes for the predictive model of customer churn using the Filter method, follow these steps:\n",
    "\n",
    "__Step 1: Data Preprocessing:__\n",
    "Ensure your dataset is properly cleaned, encoded, and prepared for analysis. Handle missing values, encode categorical variables, and scale numerical features if necessary.\n",
    "\n",
    "__Step 2: Define the Target Variable:__\n",
    "Identify the target variable, which in this case is \"customer churn.\" This variable will guide the evaluation of feature relevance.\n",
    "\n",
    "__Step 3: Calculate Feature Scores:__\n",
    "Calculate a relevance score for each feature based on its relationship with the target variable. Different scoring metrics can be used, such as correlation coefficients, ANOVA F-scores, chi-squared values, or mutual information.\n",
    "\n",
    "__Step 4: Rank Features:__\n",
    "Rank the features based on their calculated relevance scores. Features with higher scores are considered more pertinent.\n",
    "\n",
    "__Step 5: Set a Threshold:__\n",
    "Determine a threshold for feature selection. You can set a predefined threshold or choose a specific percentage of the top-ranked features to retain.\n",
    "\n",
    "__Step 6: Select Features:__\n",
    "Select the features that meet or exceed the threshold. These are the features you'll include in your predictive model.\n",
    "\n",
    "__Step 7: Create the Model:__\n",
    "Build your predictive model using the selected features. You can use various machine learning algorithms like logistic regression, decision trees, or ensemble methods.\n",
    "\n",
    "__Step 8: Evaluate the Model:__\n",
    "Evaluate the model's performance using appropriate evaluation metrics for binary classification, such as accuracy, precision, recall, F1-score, ROC-AUC, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13164c0-31d2-4273-8aa5-3e015b11b97c",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951a5d50-dd02-4038-b9ff-5ac65d5bf678",
   "metadata": {},
   "source": [
    "## Q7. You are working on a project to predict the outcome of a soccer match. You have a large dataset with many features, including player statistics and team rankings. Explain how you would use the Embedded method to select the most relevant features for the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83779181-f588-4069-ac9a-f994a5f1f365",
   "metadata": {},
   "source": [
    "## Ans: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3421a72b-f5f0-42a8-a199-681786f8efa4",
   "metadata": {},
   "source": [
    "Using the Embedded method for feature selection in soccer match outcome prediction project involves integrating feature selection within the model training process itself. Embedded methods aim to select the most relevant features while training the model, optimizing both the predictive power and the feature subset simultaneously. Here's how we could apply the Embedded method to select the most relevant features:\n",
    "\n",
    "__Step 1: Data Preprocessing:__\n",
    "Ensure your dataset is cleaned, preprocessed, and properly formatted. Handle missing values, encode categorical variables, and scale numerical features if needed.\n",
    "\n",
    "__Step 2: Data Split:__\n",
    "Split your dataset into training and testing sets to evaluate model performance later.\n",
    "\n",
    "__Step 3: Choose a Machine Learning Algorithm:__\n",
    "Select a machine learning algorithm that supports embedded feature selection. Algorithms that incorporate regularization techniques, such as Lasso regression, tree-based methods like random forests or gradient boosting, or linear models with L1 regularization, are often used in embedded feature selection.\n",
    "\n",
    "__Step 4: Model Training with Feature Selection:__\n",
    "During model training, the algorithm will automatically perform feature selection by adjusting the feature weights or coefficients based on the regularization parameter. Features with low importance will have their weights driven towards zero, effectively performing feature selection.\n",
    "\n",
    "__Step 5: Hyperparameter Tuning:__\n",
    "Tune the hyperparameters of the selected algorithm, including the regularization parameter (if applicable), to optimize model performance.\n",
    "\n",
    "__Step 6: Evaluate Model Performance:__\n",
    "Evaluate the performance of your trained model on the testing set using appropriate evaluation metrics for classification tasks, such as accuracy, precision, recall, F1-score, ROC-AUC, etc.\n",
    "\n",
    "__Step 7: Analyze Feature Importance:__\n",
    "Examine the feature importance scores provided by the algorithm (e.g., coefficients in linear models, feature importances in tree-based models). Features with higher importance scores are more relevant to the model's predictions.\n",
    "\n",
    "__Step 8: Refine the Model:__\n",
    "If necessary, further fine-tune the model, hyperparameters, or regularization strength based on the analysis of feature importance and the evaluation of model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b93d4ba-f223-45ad-ba68-f4a587a1afc4",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660ef539-0703-4b26-b5e3-e5c782290721",
   "metadata": {},
   "source": [
    "## Q8. You are working on a project to predict the price of a house based on its features, such as size, location, and age. You have a limited number of features, and you want to ensure that you select the most important ones for the model. Explain how you would use the Wrapper method to select the best set of features for the predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b06afd7-7788-4138-9bed-fef5f5fd3df0",
   "metadata": {},
   "source": [
    "## Ans: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e916dd-ca9a-405c-a926-46b4e55c7b05",
   "metadata": {},
   "source": [
    "\n",
    "Using the Wrapper method for feature selection in house price prediction project involves evaluating different subsets of features by training and testing the model on each subset. The Wrapper method uses a specific machine learning algorithm to guide the selection process and directly assess the impact of different feature combinations on the model's performance. Here's how you could apply the Wrapper method to select the best set of features:\n",
    "\n",
    "__Step 1: Data Preprocessing:__\n",
    "Prepare your dataset by handling missing values, encoding categorical variables, and scaling numerical features if necessary.\n",
    "\n",
    "__Step 2: Data Split:__\n",
    "Split your dataset into training and testing sets to evaluate model performance later.\n",
    "\n",
    "__Step 3: Choose a Machine Learning Algorithm:__\n",
    "Select a machine learning algorithm that will serve as the base model for feature selection. Algorithms like decision trees, random forests, or linear models are commonly used for Wrapper methods.\n",
    "\n",
    "__Step 4: Define a Feature Selection Criterion:__\n",
    "Choose a metric to evaluate the performance of different feature subsets. This could be a relevant evaluation metric for your specific task, such as mean squared error (MSE) for regression tasks.\n",
    "\n",
    "__Step 5: Perform Feature Subset Search:__\n",
    "Use a search strategy to explore different subsets of features. Common search strategies include forward selection, backward elimination, or recursive feature addition. Each strategy starts with an initial subset (empty or full) and iteratively adds or removes features based on their impact on the chosen evaluation metric.\n",
    "\n",
    "__Step 6: Train and Evaluate the Model:__\n",
    "For each subset of features, train the chosen machine learning algorithm on the training data and evaluate its performance on the testing data using the selected evaluation metric.\n",
    "\n",
    "__Step 7: Keep Track of Best Subset:__\n",
    "Keep track of the subset of features that results in the best model performance according to the chosen evaluation metric.\n",
    "\n",
    "__Step 8: Final Model Building:__\n",
    "Once the best feature subset is identified, build the final model using the entire training dataset and the selected features. You can further fine-tune the model's hyperparameters if needed.\n",
    "\n",
    "__Step 9: Evaluate Final Model:__\n",
    "Evaluate the performance of the final model on the testing set using the chosen evaluation metric. This gives you an estimate of how well the selected features generalize to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81ba3cb-f98b-4f33-8786-62fd3172f9e3",
   "metadata": {},
   "source": [
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
